
Got the ZIP file of my Google Transactions today . 

Current goal -> 
Collect the relevant info from  this into a structured database like SQL database 


First set up the env for my Project . ( conda create --name finance python=3.10 )
I am using 3.10 version of python gfor versatile nature and good compatibility 


My env is named =  finance 


Now write the names of all the dependencies that we wnat to install inside our NEW  env (liek numpy pandas etc )
and also use -e. to isntall local pkg  thatwe are gonna write using Py
thon OOPs 
and run it using  -> pip install -r requirements.txt 

#### IMP #####

( IMP if you are using -e . in  requoremnt.txt file , necessary to write a setup.py file 

info -> the pkg /module you install using pip ,are stored inside apython folder on C: / site-packages 
-> -e . ( e= editable packages   , . = HOEM DIR )

since all official modules  like numpy , pandas have metadta like {version ,author ,etc } , same way when we use -e . ,
we are saying to PIP  
that any function /class i will be using in this project( HOME DIR ) , should be treated same as a module like numpy pandas
and saved to site-packages folder .

so in same way to give OUR /MINE official meta- data logo to these local pkg  and
 treat it like any other official module for this or any other project in future , we use (-e .) 
 ( to add all fucntion IN  home dir. in editable mode (with metadata)  we add this metadata info. in  SETUP.PAY file) 
 whickh is like putting your OWN official seal on locla pkg YOU CREATE and saving it in your own env for curr/future USE 


)



Now I built a web scraper (using bs4.BeuatifulSoup ) to scrap transactions from the Activity.html file ( form Google takeout ZIP )
and using Regex ( using grouping -> \d to fidn single digit , \w to find single word  + (1 or multiple instances )
*( 0 or multiple instance s, () to find such groups using regex , \s fr spaces in pattern) use re.search( pattern ,text .group(1))
.group(1) to extract conte isndie it )

### MISTAKES ###

I wrote all this in my local repo first and then using git init tried to remote add origin but failed thrice , 
then made a env repo first , cloned it on my local , changed and added files , git add ,git commit and then 
git push origin main (on main branch )-> 

IMP thing i observed 
git init and git remote add origin are NOT need if we are editing a cloned repo  . 

( FORM NOW ON ,i WILL FIRST MAKE REPO and then ONLY start editing it )



#### PUT MORE FOCUS FROM HERE ON #### 

Validated Each column of the Dataset by checking it Data types and DROPPIN ROWS hwere IMPOSSIBLE dates were present ( 30 Feb 2025 )

and then split the Datset and saved it inside the Artifacts folder 


Wrote the model trainer COde 



## EDA ( most IMP ANALYSIS PART ) ##

NAlaysed the Data depely , 
like Date wise ,MErchant WIse trnaction , PEr day , PEer Quarter and tried to evalduate the NAture of Suer 

( abse don spending aptterns at diff places ) and comapred with ratio of Fialed Tranctions 


Now , today , I tracked the diff versiosn of mdoel using MLFLOW 

in model trainer code , i added a nested lop to iterate over EACH SET of hyperparams , and 
with mlflow.start_run() after eahc hyperp is laoded , trianed the modle calc theemtric 

and usin mlflow.log_param( "hyperpam name" , hypetpa value ) 
and mlflow.sklearn.log_mdoe( "mode" ,model ame) to log and save the model 

dvc init 
dvc add data csv file path ( moves ths file hypotheticlal from my old original lcotion to .dvc/cache folder )
and instead of that old location file ,a NEW file.csv.dvc ( hosted ONLINE & local but acts as a pointer to this .dvc/cache file )

and make git start trackign this csv.dvc (pointer file )  instead --> git add artifacts/trantion.cs.dvc --> git commit -m "dvc added" 


### IMP in DVC #### 
Everytime we scrap teh data or UPDATE IT ,we will NEED to add it 9jsut lie git 
DVC INIT -> DVC ADD file.csv --> DVC COMMIT --> GIT ADD file.csv.dvc  

and if by msitake we wna tto roll tback to prev bersion ,check has of prev version (in .dvc.cache)-> hash_old_ver 
and then git checkout hash --> dvc checkout (file will updated in LCOAL to to ODER version ) 

__init__.py mean --> this file can be used by ANY other file 


### Logger .__inti__.p ### 


Using consold Handelr and Dtream handler ,we will use logger file to protn info erro debugs etc 

TO use the info /erro/bug issue from logger 
use -> 
from src.logger import * 

logging.info( "INFO you want to show") after an operation is Dine (istea dof print )-> sie ts instea dof SIMPEL print staement 

try ....   except Exception as e : logging.errro( e )  raise  ->to show  teh error (use 'raise' keywrd to STOP script after error ) 

logging.debug()
logging.warning() 
etc 


and once we delet mlflow it willbe deleted form mlflow UI also and in enw rusn en dir of mlflow wil be there but will NOT 
havve OLD results 


IMP --> MY TRAINED MODEL has been saved on MLFLOW RUNS Directly 

### IMP ###
We will use mode Registry for Selecting BEst model (after saving to mlflow and  s3 using awscli at the same time )

PROBLEMS --
Mdel registry COd e( usign mlflow ) in mdoel trainer is NOT workign for NWO so leave it and the stmlit app for NOW 


### NOW TO see CI  in action ( in github actiosn) ### 

FIrst amke a .github dir / workflows dir  / ci.yaml file 
ci.yaml (containe comamnds lie pip instlal , python fiels liek pileine to run tomake sure the proejct is ASintact as it was before )


add chanegs t ANY file in repo (poject) -> git add . -> git commit -m "msg" --> 
the moemt we do git push origin main/branch ---> the Actions section of github dir -> workflows 
-> smal check itnerall done on whol project 
 it will show the cit peipeline
( the cmds we run usualy in cli terminal )
wil run and check if every come is runnig corrct giving corrct artifscts and correclty conected to nect comp . 


IMP --> USe relative apths ( NOT absolute paths ) for ci.yaml  checks 

''' IMP - the ci yaml file when rusn mlflow aain (as its runniga ll fiels agian to check conssiteny'') 
it runs ona SPEERATE  ISOLATED ubuntu env an ddoes nOT polute our own local env . 